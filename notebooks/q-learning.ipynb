{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.content = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.content)\n",
    "    \n",
    "    def insert(self, item):\n",
    "        self.content.append(item)\n",
    "        if len(self) > self.size:\n",
    "            self.content.pop(0)\n",
    "    \n",
    "    def insert_list(self, items):\n",
    "        for item in items:\n",
    "            self.insert(item)\n",
    "    \n",
    "    def can_sample(self, N):\n",
    "        return len(self) >= N\n",
    "    \n",
    "    def sample(self, N):\n",
    "        assert self.can_sample(N)\n",
    "        return random.sample(self.content, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names = [\n",
    "    'state', 'action', 'reward', 'next_state', 'done'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append(Experience(state=state, action=action, reward=reward,\n",
    "                                 next_state=next_state, done=done))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    def __init__(self, start, stop, decay=0.99):\n",
    "        self.stop  = stop\n",
    "        self.decay = decay\n",
    "        self.value = start\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.value *= self.value * self.decay\n",
    "        return max(self.value, self.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], N_HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_HIDDEN, env.action_space.n)\n",
    "        )\n",
    "        self.target = copy.deepcopy(self.net)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=LR)\n",
    "        self.epsilon_sched = Scheduler(start=1.0, stop=0.01, decay=0.99)\n",
    "    \n",
    "    def sync(self):\n",
    "        self.target.load_state_dict(self.net.state_dict())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        qvals = self.net(x)\n",
    "        return qvals\n",
    "    \n",
    "    def act(self, state):\n",
    "        epsilon = self.epsilon_sched()\n",
    "        if random.random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                qvals = self(torch.tensor([state]))[0]\n",
    "                action = qvals.max(0)[1].item()\n",
    "            return action\n",
    "        \n",
    "    \n",
    "    def update(self, batch):\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states_v = torch.tensor(states)\n",
    "        current_q = self(states_v)[range(len(batch)), actions]\n",
    "        rewards_v = torch.tensor(rewards)\n",
    "        dones_v = torch.FloatTensor(dones)\n",
    "        #set_trace()\n",
    "        \n",
    "        \n",
    "        next_states_v = torch.tensor(next_states)\n",
    "        next_q = self(next_states_v)\n",
    "        next_qmax = next_q.max(1)[0]\n",
    "        targets = rewards_v + GAMMA * (1.-dones_v) * next_qmax\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(current_q, targets.detach())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN    = 128 #24\n",
    "LR          = 0.01\n",
    "N_STEPS     = 5000\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE  = 32\n",
    "ALPHA       = 0.9\n",
    "GAMMA       = 0.99\n",
    "SYNC_RATE   = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Episode 100: Average length = 24.971665495158266\nEpisode 200: Average length = 34.78876528805114\nEpisode 300: Average length = 40.78865997197225\nEpisode 400: Average length = 46.88018176375325\nEpisode 500: Average length = 26.2813359551414\nEpisode 600: Average length = 140.71216403963194\nEpisode 700: Average length = 100.02822232694066\nEpisode 800: Average length = 124.99257685450074\nEpisode 900: Average length = 140.90898291557437\nEpisode 1000: Average length = 135.1066908595139\nEpisode 1100: Average length = 172.87360736411028\nEpisode 1200: Average length = 193.1844585228062\nEpisode 1300: Average length = 195.24356170213778\nEpisode 1400: Average length = 199.53666430579233\nEpisode 1500: Average length = 49.22004734396908\nEpisode 1600: Average length = 134.21291862054915\nEpisode 1700: Average length = 198.9894327308019\nEpisode 1800: Average length = 178.45415689228196\nEpisode 1900: Average length = 164.33875880679597\nEpisode 2000: Average length = 185.92732621951285\nEpisode 2100: Average length = 186.06909877550783\nEpisode 2200: Average length = 179.23919776683357\nEpisode 2300: Average length = 171.31177635999592\nEpisode 2400: Average length = 194.53822943784\nEpisode 2500: Average length = 190.9648107073536\nEpisode 2600: Average length = 172.71778524403297\nEpisode 2700: Average length = 182.88455826417768\nEpisode 2800: Average length = 191.66178000893788\nEpisode 2900: Average length = 193.27141508823533\nEpisode 3000: Average length = 197.3802002884825\nEpisode 3100: Average length = 191.7389617310427\nEpisode 3200: Average length = 193.3464479515146\nEpisode 3300: Average length = 164.7012376574066\nEpisode 3400: Average length = 183.92381177493067\nEpisode 3500: Average length = 173.74963267175838\nEpisode 3600: Average length = 199.97125348667691\nEpisode 3700: Average length = 195.22890133795227\nEpisode 3800: Average length = 184.8440819390269\nEpisode 3900: Average length = 199.99924914104562\nEpisode 4000: Average length = 199.97220914604256\nEpisode 4100: Average length = 199.8851355838054\nEpisode 4200: Average length = 199.99999694904048\nEpisode 4300: Average length = 199.99999999991903\nEpisode 4400: Average length = 199.892247363357\nEpisode 4500: Average length = 199.99999713793926\nEpisode 4600: Average length = 199.999999999924\nEpisode 4700: Average length = 199.67311717471398\nEpisode 4800: Average length = 199.92709131753494\nEpisode 4900: Average length = 199.99999806344343\n"
    }
   ],
   "source": [
    "env    = gym.make('CartPole-v0')\n",
    "agent  = Agent(env)\n",
    "buffer = ReplayBuffer(size=BUFFER_SIZE)\n",
    "avg_len = 10 # bookkeeping\n",
    "\n",
    "for idx in range(N_STEPS):\n",
    "    episode = generate_episode(env, agent)\n",
    "    avg_len = ALPHA * avg_len + (1.-ALPHA) * len(episode)\n",
    "    buffer.insert_list(episode)\n",
    "    if not buffer.can_sample(BATCH_SIZE):\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    agent.update(batch)\n",
    "    if idx % SYNC_RATE == 0:\n",
    "        print(f\"Episode {idx}: Average length = {avg_len}\")\n",
    "        agent.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python36964bit7ef9f40235b54a9c866fc63a0dd38a3f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
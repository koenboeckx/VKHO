{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random, copy\n",
    "import numpy as np\n",
    "import gym\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.debugger import set_trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, size):\n",
    "        self.size = size\n",
    "        self.content = []\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.content)\n",
    "    \n",
    "    def insert(self, item):\n",
    "        self.content.append(item)\n",
    "        if len(self) > self.size:\n",
    "            self.content.pop(0)\n",
    "    \n",
    "    def insert_list(self, items):\n",
    "        for item in items:\n",
    "            self.insert(item)\n",
    "    \n",
    "    def can_sample(self, N):\n",
    "        return len(self) >= N\n",
    "    \n",
    "    def sample(self, N):\n",
    "        assert self.can_sample(N)\n",
    "        return random.sample(self.content, N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Experience = namedtuple('Experience', field_names = [\n",
    "    'state', 'action', 'reward', 'next_state', 'done'\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode(env, agent):\n",
    "    episode = []\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        episode.append(Experience(state=state, action=action, reward=reward,\n",
    "                                 next_state=next_state, done=done))\n",
    "        state = next_state\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scheduler:\n",
    "    def __init__(self, start, stop, decay=0.99):\n",
    "        self.stop  = stop\n",
    "        self.decay = decay\n",
    "        self.value = start\n",
    "    \n",
    "    def __call__(self):\n",
    "        self.value *= self.value * self.decay\n",
    "        return max(self.value, self.stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent(nn.Module):\n",
    "    def __init__(self, env):\n",
    "        super().__init__()\n",
    "        self.env = env\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(env.observation_space.shape[0], N_HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_HIDDEN, N_HIDDEN),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(N_HIDDEN, env.action_space.n)\n",
    "        )\n",
    "        self.target = copy.deepcopy(self.net)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=LR)\n",
    "        self.epsilon_sched = Scheduler(start=1.0, stop=0.01, decay=0.99)\n",
    "    \n",
    "    def sync(self):\n",
    "        self.target.load_state_dict(self.net.state_dict())\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "        qvals = self.net(x)\n",
    "        return qvals\n",
    "    \n",
    "    def act(self, state):\n",
    "        epsilon = self.epsilon_sched()\n",
    "        if random.random() < epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                qvals = self(torch.tensor([state]))[0]\n",
    "                action = qvals.max(0)[1].item()\n",
    "            return action\n",
    "        \n",
    "    \n",
    "    def update(self, batch):\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        states_v = torch.tensor(states)\n",
    "        current_q = self(states_v)[range(len(batch)), actions]\n",
    "        rewards_v = torch.tensor(rewards)\n",
    "        dones_v = torch.FloatTensor(dones)\n",
    "        #set_trace()\n",
    "        \n",
    "        \n",
    "        next_states_v = torch.tensor(next_states)\n",
    "        next_q = self(next_states_v)\n",
    "        next_qmax = next_q.max(1)[0]\n",
    "        targets = rewards_v + GAMMA * (1.-dones_v) * next_qmax\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss = F.mse_loss(current_q, targets.detach())\n",
    "        loss.backward()\n",
    "        self.optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_HIDDEN    = 24\n",
    "LR          = 0.001\n",
    "N_STEPS     = 5000\n",
    "BUFFER_SIZE = 512\n",
    "BATCH_SIZE  = 16\n",
    "ALPHA       = 0.9\n",
    "GAMMA       = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Episode 0: Average length = 10.6\n",
      "Episode 100: Average length = 9.233034261716385\n",
      "Episode 200: Average length = 9.280267180750739\n",
      "Episode 300: Average length = 9.20480812958125\n",
      "Episode 400: Average length = 9.697773937391505\n",
      "Episode 500: Average length = 10.071584181887026\n",
      "Episode 600: Average length = 9.738834946775292\n",
      "Episode 700: Average length = 10.37208941168287\n",
      "Episode 800: Average length = 12.138094510292945\n",
      "Episode 900: Average length = 12.12562065936273\n",
      "Episode 1000: Average length = 16.351529297464854\n",
      "Episode 1100: Average length = 11.262563766830208\n",
      "Episode 1200: Average length = 11.415052847755396\n",
      "Episode 1300: Average length = 40.712894405851976\n",
      "Episode 1400: Average length = 18.256394458617795\n",
      "Episode 1500: Average length = 28.255564015848808\n",
      "Episode 1600: Average length = 17.323541023128726\n",
      "Episode 1700: Average length = 35.91891598968887\n",
      "Episode 1800: Average length = 27.237414954125764\n",
      "Episode 1900: Average length = 25.247698356007106\n"
     ]
    }
   ],
   "source": [
    "env    = gym.make('CartPole-v0')\n",
    "agent  = Agent(env)\n",
    "buffer = ReplayBuffer(size=BUFFER_SIZE)\n",
    "avg_len = 10 # bookkeeping\n",
    "\n",
    "for idx in range(N_STEPS):\n",
    "    episode = generate_episode(env, agent)\n",
    "    avg_len = ALPHA * avg_len + (1.-ALPHA) * len(episode)\n",
    "    buffer.insert_list(episode)\n",
    "    if not buffer.can_sample(BATCH_SIZE):\n",
    "        continue\n",
    "    batch = buffer.sample(BATCH_SIZE)\n",
    "    agent.update(batch)\n",
    "    if idx % 100 == 0:\n",
    "        print(f\"Episode {idx}: Average length = {avg_len}\")\n",
    "        agent.sync()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.6 64-bit ('base': conda)",
   "language": "python",
   "name": "python36664bitbasecondafbd257daa4084cd3977c75195d6d2ebe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
